import os
from dotenv import load_dotenv
load_dotenv()
import streamlit as st
import time
import base64
import uuid
import tempfile
from langchain_upstage import UpstageEmbeddings
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader

if "id" not in st.session_state:
    st.session_state.id = uuid.uuid4()
    st.session_state.file_cache = {}

session_id = st.session_state.id
client = None

def reset_chat():
    st.session_state.messages = []
    st.session_state.context = None


def display_pdf(file):
    # Opening file from file path

    st.markdown("### PDF Preview")
    base64_pdf = base64.b64encode(file.read()).decode("utf-8")

    # Embedding PDF in HTML
    pdf_display = f"""<iframe src="data:application/pdf;base64,{base64_pdf}" width="400" height="100%" type="application/pdf"
                        style="height:100vh; width:100%"
                    >
                    </iframe>"""

    # Displaying File
    st.markdown(pdf_display, unsafe_allow_html=True)


with st.sidebar:

    st.header(f"Add your documents!")
    
    uploaded_file = st.file_uploader("Choose your `.pdf` file", type="pdf")

    if uploaded_file:
        print(uploaded_file)
        try:
            file_key = f"{session_id}-{uploaded_file.name}"

            with tempfile.TemporaryDirectory() as temp_dir:
                file_path = os.path.join(temp_dir, uploaded_file.name)
                print("file path:", file_path)
                
                with open(file_path, "wb") as f:
                    f.write(uploaded_file.getvalue())
                
                file_key = f"{session_id}-{uploaded_file.name}"
                st.write("Indexing your document...")

                if file_key not in st.session_state.get('file_cache', {}):

                    if os.path.exists(temp_dir):
                            print("temp_dir:", temp_dir)
                            loader = PyPDFLoader(
                                file_path
                            )
                    else:    
                        st.error('Could not find the file you uploaded, please check again...')
                        st.stop()
                    
                    pages = loader.load_and_split()

                    vectorstore = Chroma.from_documents(pages, UpstageEmbeddings(model="solar-embedding-1-large"))

                    retriever = vectorstore.as_retriever(k=2)

                    from langchain_upstage import ChatUpstage
                    from langchain_core.messages import HumanMessage, SystemMessage

                    chat = ChatUpstage(upstage_api_key=os.getenv("UPSTAGE_API_KEY"))

                    # 1) ì±—ë´‡ì— 'ê¸°ì–µ'ì„ ì…íˆê¸° ìœ„í•œ ì²«ë²ˆì§¸ ë‹¨ê³„ 

                    # ì´ì „ì˜ ë©”ì‹œì§€ë“¤ê³¼ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•´, ë¬¸ë§¥ì— ëŒ€í•œ ì •ë³´ê°€ ì—†ì´ í˜¼ìì„œë§Œ ë´¤ì„ë•Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì§ˆë¬¸ì„ ë‹¤ì‹œ êµ¬ì„±í•¨
                    # ì¦‰ ìƒˆë¡œ ë“¤ì–´ì˜¨ ê·¸ ì§ˆë¬¸ ìì²´ì—ë§Œ ì§‘ì¤‘í•  ìˆ˜ ìˆë„ë¡ ë‹¤ì‹œ ì¬í¸ì„±
                    from langchain.chains import create_history_aware_retriever
                    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

                    contextualize_q_system_prompt = """ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ì´ ìˆì„ ë•Œ, ì´ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
                    ì´ëŸ° ê²½ìš°, ëŒ€í™” ë‚´ìš©ì„ ì•Œ í•„ìš” ì—†ì´ ë…ë¦½ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ìœ¼ë¡œ ë°”ê¾¸ì„¸ìš”. 
                    ì§ˆë¬¸ì— ë‹µí•  í•„ìš”ëŠ” ì—†ê³ , í•„ìš”í•˜ë‹¤ë©´ ê·¸ì € ë‹¤ì‹œ êµ¬ì„±í•˜ê±°ë‚˜ ê·¸ëŒ€ë¡œ ë‘ì„¸ìš”."""

                    # MessagesPlaceholder: 'chat_history' ì…ë ¥ í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ì „ ë©”ì„¸ì§€ ê¸°ë¡ë“¤ì„ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ì‹œí‚´.
                    # ì¦‰ í”„ë¡¬í”„íŠ¸, ë©”ì„¸ì§€ ê¸°ë¡ (ë¬¸ë§¥ ì •ë³´), ì‚¬ìš©ìì˜ ì§ˆë¬¸ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ê°€ êµ¬ì„±ë¨. 
                    contextualize_q_prompt = ChatPromptTemplate.from_messages(
                        [
                            ("system", contextualize_q_system_prompt),
                            MessagesPlaceholder("chat_history"),
                            ("human", "{input}"),
                        ]
                    )

                    # ì´ë¥¼ í† ëŒ€ë¡œ ë©”ì„¸ì§€ ê¸°ë¡ì„ ê¸°ì–µí•˜ëŠ” retrieverë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
                    history_aware_retriever = create_history_aware_retriever(
                        chat, retriever, contextualize_q_prompt
                    )

                    # 2) ë‘ë²ˆì§¸ ë‹¨ê³„ë¡œ, ë°©ê¸ˆ ì „ ìƒì„±í•œ ì²´ì¸ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆëŠ” retriever ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
                    from langchain.chains import create_retrieval_chain
                    from langchain.chains.combine_documents import create_stuff_documents_chain

                    qa_system_prompt = """ì§ˆë¬¸-ë‹µë³€ ì—…ë¬´ë¥¼ ë•ëŠ” ë³´ì¡°ì›ì…ë‹ˆë‹¤. 
                    ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ ê²€ìƒ‰ëœ ë‚´ìš©ì„ ì‚¬ìš©í•˜ì„¸ìš”. 
                    ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”. 
                    ë‹µë³€ì€ ì„¸ ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ìœ ì§€í•˜ì„¸ìš”.

                    ## ë‹µë³€ ì˜ˆì‹œ
                    ğŸ“ë‹µë³€ ë‚´ìš©: 
                    ğŸ“ì¦ê±°: 

                    {context}"""
                    qa_prompt = ChatPromptTemplate.from_messages(
                        [
                            ("system", qa_system_prompt),
                            MessagesPlaceholder("chat_history"),
                            ("human", "{input}"),
                        ]
                    )

                    question_answer_chain = create_stuff_documents_chain(chat, qa_prompt)

                    # ê²°ê³¼ê°’ì€ input, chat_history, context, answer í¬í•¨í•¨.
                    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

                st.success("Ready to Chat!")
                display_pdf(uploaded_file)
        except Exception as e:
            st.error(f"An error occurred: {e}")
            st.stop()     

# ì›¹ì‚¬ì´íŠ¸ ì œëª©
st.title("Solar LLM Chatbot")

if "openai_model" not in st.session_state:
    st.session_state["openai_model"] = "gpt-3.5-turbo"

if "messages" not in st.session_state:
    st.session_state.messages = []

# ëŒ€í™” ë‚´ìš©ì„ ê¸°ë¡í•˜ê¸° ìœ„í•´ ì…‹ì—…
# Streamlit íŠ¹ì„±ìƒ í™œì„±í™”í•˜ì§€ ì•Šìœ¼ë©´ ë‚´ìš©ì´ ë‹¤ ë‚ ì•„ê°.
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        
# í”„ë¡¬í”„íŠ¸ ë¹„ìš©ì´ ë„ˆë¬´ ë§ì´ ì†Œìš”ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´
MAX_MESSAGES_BEFORE_DELETION = 4

# ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìœ ì €ì˜ ì¸í’‹ì„ ë°›ê³  ìœ„ì—ì„œ ë§Œë“  AI ì—ì´ì „íŠ¸ ì‹¤í–‰ì‹œì¼œì„œ ë‹µë³€ ë°›ê¸°
if prompt := st.chat_input("Ask a question!"):
    
# ìœ ì €ê°€ ë³´ë‚¸ ì§ˆë¬¸ì´ë©´ ìœ ì € ì•„ì´ì½˜ê³¼ ì§ˆë¬¸ ë³´ì—¬ì£¼ê¸°
     # ë§Œì•½ í˜„ì¬ ì €ì¥ëœ ëŒ€í™” ë‚´ìš© ê¸°ë¡ì´ 4ê°œë³´ë‹¤ ë§ìœ¼ë©´ ìë¥´ê¸°
    if len(st.session_state.messages) >= MAX_MESSAGES_BEFORE_DELETION:
        # Remove the first two messages
        del st.session_state.messages[0]
        del st.session_state.messages[0]  
   
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

# AIê°€ ë³´ë‚¸ ë‹µë³€ì´ë©´ AI ì•„ì´ì½˜ì´ë‘ LLM ì‹¤í–‰ì‹œì¼œì„œ ë‹µë³€ ë°›ê³  ìŠ¤íŠ¸ë¦¬ë°í•´ì„œ ë³´ì—¬ì£¼ê¸°
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        result = rag_chain.invoke({"input": prompt, "chat_history": st.session_state.messages})

        # ì¦ê±°ìë£Œ ë³´ì—¬ì£¼ê¸°
        with st.expander("Evidence context"):
            st.write(result["context"])

        for chunk in result["answer"].split(" "):
            full_response += chunk + " "
            time.sleep(0.2)
            message_placeholder.markdown(full_response + "â–Œ")
            message_placeholder.markdown(full_response)
            
    st.session_state.messages.append({"role": "assistant", "content": full_response})

print("_______________________")
print(st.session_state.messages)
